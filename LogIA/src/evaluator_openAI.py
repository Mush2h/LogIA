import os
import json
from llama_index.llms.openai import OpenAI as LlamaOpenAI

class Evaluator:
    def __init__(self, answers_dict, openai_api_key, reference_model=None, ground_truth=None):
        self.answers = answers_dict
        self.reference_model = reference_model
        self.llm = LlamaOpenAI(model="gpt-4", temperature=0.3, api_key=openai_api_key)

        self.gold_answer = None
        self.base_prompt = ""

        # GPT vs GPT mode
        if reference_model:
            if reference_model not in answers_dict:
                raise ValueError(f"Reference model answer '{reference_model}' not found.")
            self.gold_answer = self._extract_answer(answers_dict[reference_model])
            self.base_prompt = answers_dict[reference_model].get("prompt", "")

        # Ground Truth mode
        self.ground_truth = ground_truth

    def _extract_answer(self, data):
        """
        Returns the model's answer in unified text format.
        """
        if "answer" in data:
            return data["answer"]
        elif "questions_answers" in data:
            qa = data["questions_answers"]
            return "\n".join([f"- {r}" for r in qa.values()])
        else:
            return "(No answer available)"

    def _build_eval_prompt_gpt_vs_gpt(self, model_answer, model_name):
        prompt = f"""
You are a cybersecurity expert and an evaluator of answers generated by language models.

You are given a series of log events and a set of questions. GPT-4 has already generated a correct reference answer (gold answer), and now you are asked to evaluate another answer generated by the model **{model_name}**, comparing it to the reference.

Evaluate to what extent the model's answer matches the correct one, pointing out:
1. What it gets right and why.
2. What it gets wrong and why.
3. Whether the answer is correct or incorrect.
4. An overall grade from 0 to 10.

--- ORIGINAL PROMPT ---
{self.base_prompt}

--- REFERENCE ANSWER (GPT-4) ---
{self.gold_answer}

--- MODEL ANSWER {model_name} ---
{model_answer}

--- EVALUATION (by GPT-4) ---
"""
        return prompt.strip()

    def _build_eval_prompt_vs_groundtruth(self, model_data):
        topic = model_data.get("topic")
        model_answers = model_data.get("questions_answers", {})
        expected_answers = self.ground_truth.get(topic, {})

        prompt = f"""
You are an expert evaluator. Below you are presented with a set of questions, the correct answers (ground truth), and the answers given by a language model.

Your task is to compare each model answer with the correct one and point out:
1. Whether it is correct or incorrect.
2. A brief explanation if it is incorrect.
3. A score from 0 to 10 based on the number of correct answers.

--- TOPIC ---
{topic}

--- CORRECT ANSWERS ---
{json.dumps(expected_answers, indent=2, ensure_ascii=False)}

--- MODEL ANSWERS ---
{json.dumps(model_answers, indent=2, ensure_ascii=False)}

--- EVALUATION ---
"""
        return prompt.strip()

    def evaluate_models_with_openai(self):
        """GPT vs GPT-4"""
        results = {}
        for model, data in self.answers.items():
            if model == self.reference_model:
                continue

            model_answer = self._extract_answer(data)
            prompt = self._build_eval_prompt_gpt_vs_gpt(model_answer, model)
            print(f"üß† Evaluating model: {model} with GPT-4...")

            try:
                evaluation = self.llm.complete(prompt).text.strip()
                results[model] = {
                    "model": model,
                    "evaluation": evaluation,
                    "file": data.get("file", ""),
                    "topic": data.get("topic", "")
                }
            except Exception as e:
                print(f"‚ùå Error evaluating model {model}: {e}")
                results[model] = {
                    "model": model,
                    "error": str(e)
                }

        return results

    def evaluate_models_with_groundtruth_openai(self):
        """GPT evaluates vs ground truth"""
        if not self.ground_truth:
            raise ValueError("You must provide ground_truth for this evaluation.")

        results = {}
        for model, data in self.answers.items():
            if not data.get("questions_answers"):
                continue

            prompt = self._build_eval_prompt_vs_groundtruth(data)
            print(f"üß† Evaluating model: {model} with GPT-4 comparing against ground truth...")

            try:
                evaluation = self.llm.complete(prompt).text.strip()
                results[model] = {
                    "model": model,
                    "evaluation": evaluation,
                    "file": data.get("file", ""),
                    "topic": data.get("topic", "")
                }
            except Exception as e:
                print(f"‚ùå Error evaluating model {model}: {e}")
                results[model] = {
                    "model": model,
                    "error": str(e)
                }

        return results
