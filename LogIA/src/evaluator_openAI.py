import os
import json
from llama_index.llms.openai import OpenAI as LlamaOpenAI

class Evaluator:
    """
    Evaluation class for comparing model-generated answers using GPT-4.

    Supports two evaluation modes:
    1. GPT vs GPT: Compare candidate model answers against a GPT-4 reference answer.
    2. GPT vs Ground Truth: Compare candidate model answers against a predefined ground truth.

    Attributes:
        answers (dict): Dictionary with model responses.
        reference_model (str): Name of the reference model for GPT vs GPT evaluation.
        llm (LlamaOpenAI): GPT-4 client for evaluation prompts.
        gold_answer (str): Reference (gold) answer extracted from GPT-4 or another reference model.
        base_prompt (str): Original prompt used to generate the reference model answer.
        ground_truth (dict): Ground truth dataset for evaluation.
    """

    def __init__(self, answers_dict, openai_api_key, reference_model=None, ground_truth=None):
        """
        Initialize the evaluator.

        Args:
            answers_dict (dict): Dictionary containing model answers.
            openai_api_key (str): OpenAI API key for GPT-4 access.
            reference_model (str, optional): Reference model name (for GPT vs GPT).
            ground_truth (dict, optional): Ground truth answers (for GPT vs Ground Truth).
        """
        self.answers = answers_dict
        self.reference_model = reference_model
        self.llm = LlamaOpenAI(model="gpt-4", temperature=0.3, api_key=openai_api_key)

        self.gold_answer = None
        self.base_prompt = ""

        # GPT vs GPT mode: extract the reference answer
        if reference_model:
            if reference_model not in answers_dict:
                raise ValueError(f"Reference model answer '{reference_model}' not found.")
            self.gold_answer = self._extract_answer(answers_dict[reference_model])
            self.base_prompt = answers_dict[reference_model].get("prompt", "")

        # Ground Truth mode
        self.ground_truth = ground_truth


    # üîß Utility methods -------------------------------------------------------

    def _extract_answer(self, data):
        """
        Extract the model's answer in a unified text format.

        Args:
            data (dict): Model response data.

        Returns:
            str: Unified answer text.
        """
        if "answer" in data:
            return data["answer"]
        elif "questions_answers" in data:
            qa = data["questions_answers"]
            return "\n".join([f"- {r}" for r in qa.values()])
        else:
            return "(No answer available)"


    def _build_eval_prompt_gpt_vs_gpt(self, model_answer, model_name):
        """
        Build a GPT-4 evaluation prompt for GPT vs GPT mode.

        Args:
            model_answer (str): Candidate model answer.
            model_name (str): Candidate model name.

        Returns:
            str: Evaluation prompt for GPT-4.
        """
        prompt = f"""
You are a cybersecurity expert and an evaluator of answers generated by language models.

You are given a series of log events and a set of questions. GPT-4 has already generated a correct reference answer (gold answer), and now you are asked to evaluate another answer generated by the model **{model_name}**, comparing it to the reference.

Evaluate to what extent the model's answer matches the correct one, pointing out:
1. What it gets right and why.
2. What it gets wrong and why.
3. Whether the answer is correct or incorrect.
4. An overall grade from 0 to 10.

--- ORIGINAL PROMPT ---
{self.base_prompt}

--- REFERENCE ANSWER (GPT-4) ---
{self.gold_answer}

--- MODEL ANSWER {model_name} ---
{model_answer}

--- EVALUATION (by GPT-4) ---
"""
        return prompt.strip()


    def _build_eval_prompt_vs_groundtruth(self, model_data):
        """
        Build a GPT-4 evaluation prompt for GPT vs Ground Truth mode.

        Args:
            model_data (dict): Candidate model response.

        Returns:
            str: Evaluation prompt for GPT-4.
        """
        topic = model_data.get("topic")
        model_answers = model_data.get("questions_answers", {})
        expected_answers = self.ground_truth.get(topic, {})

        prompt = f"""
You are an expert evaluator. Below you are presented with a set of questions, the correct answers (ground truth), and the answers given by a language model.

Your task is to compare each model answer with the correct one and point out:
1. Whether it is correct or incorrect.
2. A brief explanation if it is incorrect.
3. A score from 0 to 10 based on the number of correct answers.

--- TOPIC ---
{topic}

--- CORRECT ANSWERS ---
{json.dumps(expected_answers, indent=2, ensure_ascii=False)}

--- MODEL ANSWERS ---
{json.dumps(model_answers, indent=2, ensure_ascii=False)}

--- EVALUATION ---
"""
        return prompt.strip()


    # üöÄ Evaluation methods ---------------------------------------------------

    def evaluate_models_with_openai(self):
        """
        Evaluate candidate models against a reference model (GPT vs GPT).

        Returns:
            dict: Evaluation results, keyed by model name.
        """
        results = {}
        for model, data in self.answers.items():
            if model == self.reference_model:
                continue

            model_answer = self._extract_answer(data)
            prompt = self._build_eval_prompt_gpt_vs_gpt(model_answer, model)
            print(f"üß† Evaluating model: {model} with GPT-4...")

            try:
                evaluation = self.llm.complete(prompt).text.strip()
                results[model] = {
                    "model": model,
                    "evaluation": evaluation,
                    "file": data.get("file", ""),
                    "topic": data.get("topic", "")
                }
            except Exception as e:
                print(f"‚ùå Error evaluating model {model}: {e}")
                results[model] = {
                    "model": model,
                    "error": str(e)
                }

        return results


    def evaluate_models_with_groundtruth_openai(self):
        """
        Evaluate candidate models against ground truth (GPT vs Ground Truth).

        Returns:
            dict: Evaluation results, keyed by model name.

        Raises:
            ValueError: If no ground truth was provided.
        """
        if not self.ground_truth:
            raise ValueError("You must provide ground_truth for this evaluation.")

        results = {}
        for model, data in self.answers.items():
            if not data.get("questions_answers"):
                continue

            prompt = self._build_eval_prompt_vs_groundtruth(data)
            print(f"üß† Evaluating model: {model} with GPT-4 comparing against ground truth...")

            try:
                evaluation = self.llm.complete(prompt).text.strip()
                results[model] = {
                    "model": model,
                    "evaluation": evaluation,
                    "file": data.get("file", ""),
                    "topic": data.get("topic", "")
                }
            except Exception as e:
                print(f"‚ùå Error evaluating model {model}: {e}")
                results[model] = {
                    "model": model,
                    "error": str(e)
                }

        return results
