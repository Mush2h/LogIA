
# ğŸ“‘ Log Analysis & LLM Model Evaluation Framework


This repository provides a framework for **parsing real and simulated logs**, generating **responses from multiple LLMs**, and **evaluating model performance** against a ground truth dataset.  
It is designed for research and experimentation in the intersection of **log analysis, AI-assisted cybersecurity, and model benchmarking**.

---

## ğŸ“‚ Repository Structure

```
â”œâ”€â”€ data/                           # Real and simulated datasets
â”‚   â”œâ”€â”€ real_events.csv
â”‚   â”œâ”€â”€ real_parsed_logs_all.json
â”‚   â”œâ”€â”€ real_parsed_logs_by_unique_rule_description.json
â”‚   â”œâ”€â”€ real_parsed_logs_filtered.json
â”‚   â”œâ”€â”€ simulated_events.csv
â”‚   â”œâ”€â”€ simulated_parsed_logs_all.json
â”‚   â”œâ”€â”€ simulated_parsed_logs_by_unique_rule_description.json
â”‚   â””â”€â”€ simulated_parsed_logs_filtered.json
â”‚
â”œâ”€â”€ eval/
â”‚   â””â”€â”€ ground_truth_simulated.json # Ground truth for evaluation
â”‚
â”œâ”€â”€ evaluate_models.py              # Main evaluation script
â”œâ”€â”€ generate_model_all_topics.py    # Generate responses for all topics
â”œâ”€â”€ generate_model_responses_menu.py# Generate responses via interactive menu
â”‚
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ parse_logs.py               # Log parsing utilities
â”‚   â””â”€â”€ __pycache__/                # Compiled Python files
â”‚
â”œâ”€â”€ model_responses/                # Generated model responses (JSON format)
â”‚
â”œâ”€â”€ requirements.txt                # Python dependencies
â”‚
â”œâ”€â”€ responses_by_topic/             # Responses organized by topic
â”‚   â””â”€â”€ topic_1_-_basic_events/
â”‚       â”œâ”€â”€ response_deepseek-r1_32b_20250917_130244.json
â”‚       â”œâ”€â”€ response_llama3.2_20250917_130244.json
â”‚       â”œâ”€â”€ response_openai_gpt4_20250917_130157.json
â”‚       â”œâ”€â”€ response_openai_gpt4_20250917_130244.json
â”‚       â””â”€â”€ response_phi4_20250917_130244.json
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ dataset.py                  # Dataset handling and preprocessing
    â”œâ”€â”€ evaluator_human.py          # Human-based evaluation
    â”œâ”€â”€ evaluator_openAI.py         # Automated evaluation with OpenAI
    â””â”€â”€ __pycache__/                # Compiled Python files
```

---

## âš™ï¸ Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/Mush2h/LogIA 
   cd LogIA
   ```

2. (Optional but recommended) Create a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate   # Linux/Mac
   venv\Scripts\activate      # Windows
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ§‘â€ğŸ’» Usage

### 1. Generate Model Responses
- Generate responses for **all topics**:
  ```bash
  python generate_model_all_topics.py
  ```

- Generate responses via **interactive menu**:
  ```bash
  python generate_model_responses_menu.py
  ```

### 2. Evaluate Model Performance
- Compare generated responses against the ground truth:
  ```bash
  python evaluate_models.py
  ```

### 3. Log Parsing
- Import and use the log parsing utilities:
  ```python
  from lib.parse_logs import parse_logs
  ```

---

## ğŸ“Š Evaluation Methodology

- **Automated Evaluation**: `evaluator_openAI.py` leverages LLMs for consistency scoring.  
- **Human Evaluation**: `evaluator_human.py` enables manual assessment for subjective or qualitative aspects.  
- **Datasets**: Located under `data/`  
- **Ground Truth**: Defined in `eval/ground_truth_simulated.json`  

This dual evaluation approach ensures both **quantitative benchmarking** and **qualitative insights**.

---

## ğŸ¤ Contributing

Contributions are welcome!  
To contribute:  

1. Fork this repository  
2. Create a new branch:  
   ```bash
   git checkout -b feature-name
   ```
3. Commit your changes  
4. Open a Pull Request ğŸš€  

---

## ğŸ“œ License

This project is distributed under the **MIT License**.  
See the [LICENSE](LICENSE) file for details.
